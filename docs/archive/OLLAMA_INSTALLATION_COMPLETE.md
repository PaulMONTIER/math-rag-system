# Installation Ollama et Mode Hybride - R√©sum√© Complet

## Date: 2025-11-17

## R√©sum√©

Installation compl√®te d'Ollama (mod√®le local open-source) et mise en place du s√©lecteur de mod√®le LLM avec 3 modes de fonctionnement, incluant un mode hybride intelligent.

---

## ‚úÖ Installation Ollama R√©ussie

### √âtapes d'installation effectu√©es

1. **V√©rification Homebrew**
   ```bash
   which brew
   # Output: /opt/homebrew/bin/brew
   ```

2. **Installation Ollama**
   ```bash
   brew install ollama
   ```
   - Version install√©e: **0.12.11**
   - Taille: 29.4 MB

3. **D√©marrage du service Ollama**
   ```bash
   brew services start ollama
   ```
   - Service actif sur `localhost:11434`
   - D√©marrage automatique au boot syst√®me

4. **T√©l√©chargement du mod√®le Mistral**
   ```bash
   ollama pull mistral
   ```
   - Mod√®le: **mistral:latest**
   - Taille: **4.4 GB**
   - Param√®tres: **7.2B**
   - Quantification: **Q4_K_M** (format GGUF)
   - Famille: **llama**

### V√©rification de l'installation

```bash
# Lister les mod√®les install√©s
ollama list
# Output: mistral:latest    6577803aa9a0    4.4 GB    2 minutes ago

# Tester l'API
curl http://localhost:11434/api/tags
# Output: JSON avec liste des mod√®les disponibles
```

---

## üéØ Fonctionnalit√©s Impl√©ment√©es

### S√©lecteur de Mod√®le LLM

**Emplacement**: Panneau lat√©ral gauche de l'interface Streamlit

**3 modes disponibles**:

#### 1. Mod√®le ferm√© (GPT-4o)
- Utilise **UNIQUEMENT** GPT-4o d'OpenAI
- Meilleure qualit√© de raisonnement
- Pr√©cision math√©matique maximale
- N√©cessite une connexion Internet et cl√© API OpenAI

#### 2. Mod√®le ouvert (Ollama)
- Utilise **UNIQUEMENT** le mod√®le local Mistral via Ollama
- Fonctionne enti√®rement en local (pas de connexion Internet n√©cessaire)
- Plus rapide pour des t√¢ches simples
- Confidentialit√© totale (aucune donn√©e envoy√©e √† l'ext√©rieur)

#### 3. Les deux (combinaison)
- **Mode hybride intelligent** o√π les deux mod√®les collaborent
- Workflow en 2 √©tapes:
  1. **Ollama g√©n√®re le brouillon** (rapide, local)
  2. **GPT-4o raffine la r√©ponse** (haute qualit√©)
- Combine les avantages des deux mod√®les
- Sources fusionn√©es et d√©dupliqu√©es

---

## üîÑ Workflow du Mode Hybride

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         S√âLECTION: "Les deux (combinaison)"          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  √âTAPE 1: G√©n√©ration Brouillon (Ollama/Mistral)     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ Classification de la question                     ‚îÇ
‚îÇ  ‚Ä¢ Recherche de documents pertinents                 ‚îÇ
‚îÇ  ‚Ä¢ G√©n√©ration de la r√©ponse initiale                 ‚îÇ
‚îÇ  ‚Ä¢ Extraction des sources (draft_sources)            ‚îÇ
‚îÇ  üìù "G√©n√©ration du brouillon (mod√®le ouvert)..."     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    √âTAPE 2: Raffinement (GPT-4o)                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ R√©ception du brouillon d'Ollama                   ‚îÇ
‚îÇ  ‚Ä¢ V√©rification de l'exactitude math√©matique         ‚îÇ
‚îÇ  ‚Ä¢ Am√©lioration de la clart√© et pr√©cision            ‚îÇ
‚îÇ  ‚Ä¢ Enrichissement des explications                   ‚îÇ
‚îÇ  ‚Ä¢ G√©n√©ration de la r√©ponse finale                   ‚îÇ
‚îÇ  ‚Ä¢ Extraction des sources (refined_sources)          ‚îÇ
‚îÇ  ‚ú® "Raffinement de la r√©ponse (mod√®le ferm√©)..."    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          COMBINAISON DES R√âSULTATS                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ R√©ponse finale = version raffin√©e par GPT-4o      ‚îÇ
‚îÇ  ‚Ä¢ Sources = fusion(draft_sources, refined_sources)  ‚îÇ
‚îÇ  ‚Ä¢ D√©duplication automatique des sources             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           AFFICHAGE √Ä L'UTILISATEUR                  ‚îÇ
‚îÇ  ‚ÑπÔ∏è Mode hybride activ√© : Brouillon g√©n√©r√© par      ‚îÇ
‚îÇ     Ollama, raffin√© par GPT-4o                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ°Ô∏è Gestion des Erreurs

### Si Ollama n'est pas disponible

Le syst√®me d√©tecte automatiquement la disponibilit√© d'Ollama au d√©marrage.

**Cas 1: Ollama non disponible au d√©marrage**
- Affichage d'un warning: `‚ö†Ô∏è Ollama non disponible. Seul GPT-4o est utilisable.`
- Le s√©lecteur n'affiche que l'option "Mod√®le ferm√© (GPT-4o)"

**Cas 2: Mode hybride s√©lectionn√© mais Ollama √©choue**
- Message: `‚ö†Ô∏è Mod√®le ouvert indisponible, utilisation du mod√®le ferm√©...`
- Bascule automatique sur GPT-4o uniquement
- Aucune interruption du service

---

## üìÇ Fichiers Modifi√©s

### 1. src/workflow/langgraph_pipeline.py

**Lignes 343-369**: Ajout du param√®tre `force_provider`

```python
def create_rag_workflow(config: object, force_provider: Optional[str] = None) -> Any:
    """
    Cr√©e le workflow LangGraph complet.

    Args:
        config: Objet Config
        force_provider: Provider LLM √† utiliser (override config.llm.provider)
                       Options: "openai", "local"

    Returns:
        Workflow compil√© pr√™t √† l'emploi
    """
    provider_info = f" with provider={force_provider}" if force_provider else ""
    logger.info(f"Creating RAG workflow{provider_info}")

    # Initialiser composants
    cost_tracker = CostTracker(config)
    llm_client = get_llm_client(config, cost_tracker, force_provider=force_provider)
    # ... reste du code
```

### 2. src/interface/app.py

#### a) Initialisation des workflows (lignes 572-602)

Cr√©ation de workflows s√©par√©s pour chaque provider:

```python
@st.cache_resource
def init_system():
    """
    Initialise le syst√®me (une seule fois).
    Cr√©e plusieurs workflows pour diff√©rents providers LLM.
    """
    with st.spinner("‚è≥ Initialisation du syst√®me..."):
        try:
            config = load_config()
            workflows = {}

            # GPT-4o (OpenAI) - Mod√®le ferm√©
            workflows["openai"] = create_rag_workflow(config, force_provider="openai")

            # Ollama (local) - Mod√®le ouvert
            try:
                workflows["local"] = create_rag_workflow(config, force_provider="local")
            except Exception as e:
                workflows["local"] = None  # Ollama non disponible

            return config, workflows
        except Exception as e:
            st.error(f"‚ùå Erreur d'initialisation: {e}")
            st.stop()
```

#### b) S√©lecteur dans le sidebar (lignes 646-672)

```python
with st.sidebar:
    st.markdown("### Mod√®le de g√©n√©ration")

    # D√©terminer les options disponibles
    available_options = ["Mod√®le ferm√© (GPT-4o)"]

    if workflows.get("local") is not None:
        available_options.extend([
            "Mod√®le ouvert (Ollama)",
            "Les deux (combinaison)"
        ])
    else:
        st.warning("‚ö†Ô∏è Ollama non disponible. Seul GPT-4o est utilisable.")

    llm_choice = st.selectbox(
        "Choisir le type de mod√®le",
        available_options,
        index=0,
        label_visibility="collapsed",
        help="Mod√®le ferm√©: GPT-4o uniquement | Mod√®le ouvert: Ollama uniquement | Les deux: combinaison intelligente"
    )
```

#### c) Logique de s√©lection (lignes 700-728)

Mapping du choix utilisateur vers le provider:

```python
llm_choice_to_provider = {
    "Mod√®le ferm√© (GPT-4o)": "openai",
    "Mod√®le ouvert (Ollama)": "local",
    "Les deux (combinaison)": "hybrid"
}
provider = llm_choice_to_provider.get(llm_choice, "openai")

if provider == "hybrid":
    workflow_open = workflows.get("local")
    workflow_closed = workflows.get("openai")

    if workflow_open is None:
        st.warning("‚ö†Ô∏è Mod√®le ouvert (Ollama) non disponible. Utilisation de GPT-4o uniquement.")
        workflow_1 = workflow_closed
        hybrid_mode = False
    else:
        workflow_1 = workflow_closed
        hybrid_mode = True
else:
    workflow_1 = workflows.get(provider)
    hybrid_mode = False
```

#### d) Workflow hybride (lignes 745-816)

Impl√©mentation compl√®te du mode hybride avec gestion d'erreurs.

#### e) Affichage du statut syst√®me (lignes 752-767)

```python
st.markdown("### Syst√®me")
providers_available = []
if workflows.get("openai"):
    providers_available.append("‚úÖ GPT-4o")
if workflows.get("local"):
    providers_available.append("‚úÖ Ollama")
else:
    providers_available.append("‚ùå Ollama")

st.caption(f"**Providers disponibles:** {', '.join(providers_available)}")
```

---

## üß™ Comment Tester

### 1. Actualiser l'interface Streamlit

Acc√©dez √† l'une de ces URLs dans votre navigateur:

- **Local**: http://localhost:8501
- **R√©seau**: http://192.168.1.82:8501
- **Externe**: http://37.65.162.11:8501

### 2. V√©rifier le statut dans le sidebar

Dans le panneau de gauche, section "Syst√®me", vous devriez voir:

```
Providers disponibles: ‚úÖ GPT-4o, ‚úÖ Ollama
```

### 3. Tester chaque mode

#### Test du mode "Mod√®le ouvert (Ollama)"

1. S√©lectionner "Mod√®le ouvert (Ollama)" dans le s√©lecteur
2. Poser une question simple: "Qu'est-ce qu'une d√©riv√©e ?"
3. Observer la g√©n√©ration locale (rapide, sans appel API externe)

#### Test du mode "Mod√®le ferm√© (GPT-4o)"

1. S√©lectionner "Mod√®le ferm√© (GPT-4o)"
2. Poser une question math√©matique complexe
3. Observer la g√©n√©ration de haute qualit√© par GPT-4o

#### Test du mode hybride "Les deux (combinaison)"

1. S√©lectionner "Les deux (combinaison)"
2. Poser une question: "Qu'est-ce qu'une int√©grale d√©finie ?"
3. Observer les 2 √©tapes:
   - `üìù G√©n√©ration du brouillon (mod√®le ouvert)...`
   - `‚ú® Raffinement de la r√©ponse (mod√®le ferm√©)...`
4. V√©rifier le message: `‚ÑπÔ∏è Mode hybride activ√© : Brouillon g√©n√©r√© par Ollama, raffin√© par GPT-4o`

---

## üìä Avantages du Mode Hybride

| Aspect | Avantage |
|--------|----------|
| **Rapidit√©** | Le mod√®le ouvert g√©n√®re rapidement un brouillon de qualit√© (local, pas de latence r√©seau) |
| **Qualit√©** | Le mod√®le ferm√© raffine pour une exactitude maximale |
| **Co√ªt** | Optimisation des co√ªts en utilisant le mod√®le local pour le travail initial |
| **Confidentialit√©** | Premi√®re passe en local, raffinement avec donn√©es d√©j√† trait√©es |
| **Robustesse** | Bascule automatique si le mod√®le ouvert n'est pas disponible |
| **Transparence** | L'utilisateur voit clairement quel mode est actif |
| **Sources** | Combinaison et d√©duplication des sources des deux mod√®les |

---

## ‚öôÔ∏è Configuration Requise

### Pour le mode "Mod√®le ferm√© (GPT-4o)"
- ‚úÖ Cl√© API OpenAI configur√©e dans `config/config.yaml`
- ‚úÖ Connexion Internet active

### Pour le mode "Mod√®le ouvert (Ollama)"
- ‚úÖ Ollama install√© via Homebrew
- ‚úÖ Service Ollama d√©marr√©: `brew services start ollama`
- ‚úÖ Mod√®le Mistral t√©l√©charg√©: `ollama pull mistral`
- ‚úÖ Serveur Ollama accessible sur `localhost:11434`

### Pour le mode "Les deux (combinaison)"
- ‚úÖ Les deux configurations ci-dessus
- ‚úÖ Si Ollama n'est pas disponible, bascule automatique sur GPT-4o uniquement

---

## üîß Commandes Utiles Ollama

### Gestion du service

```bash
# D√©marrer Ollama
brew services start ollama

# Arr√™ter Ollama
brew services stop ollama

# Red√©marrer Ollama
brew services restart ollama

# V√©rifier le statut
brew services list | grep ollama
```

### Gestion des mod√®les

```bash
# Lister les mod√®les install√©s
ollama list

# T√©l√©charger un nouveau mod√®le
ollama pull <nom_modele>

# Supprimer un mod√®le
ollama rm <nom_modele>

# Tester un mod√®le en ligne de commande
ollama run mistral "Explique ce qu'est une d√©riv√©e"
```

### V√©rification API

```bash
# V√©rifier que l'API est accessible
curl http://localhost:11434/api/tags

# Lister les mod√®les via API
curl http://localhost:11434/api/tags | python3 -m json.tool
```

---

## üêõ D√©pannage

### Probl√®me: "Ollama non disponible" dans l'interface

**Solution 1**: V√©rifier que le service Ollama est d√©marr√©
```bash
brew services list | grep ollama
# Si "stopped", ex√©cuter:
brew services start ollama
```

**Solution 2**: V√©rifier que l'API r√©pond
```bash
curl http://localhost:11434/api/tags
# Doit retourner un JSON avec la liste des mod√®les
```

**Solution 3**: Red√©marrer Streamlit
```bash
pkill -f "streamlit run"
# Puis relancer l'application
```

### Probl√®me: Mode hybride utilise uniquement GPT-4o

**Cause**: Ollama a √©chou√© pendant la g√©n√©ration du brouillon

**Solution**: V√©rifier les logs Ollama
```bash
# Logs du service
brew services info ollama

# Tester manuellement
ollama run mistral "Test"
```

### Probl√®me: G√©n√©ration lente avec Ollama

**Cause**: Mistral (7.2B param√®tres) n√©cessite des ressources CPU/GPU

**Solutions**:
- Utiliser un mod√®le plus l√©ger: `ollama pull phi` (2.7B param√®tres)
- Utiliser le mode "Mod√®le ferm√© (GPT-4o)" pour les questions complexes
- Utiliser le mode hybride pour combiner rapidit√© locale et qualit√© cloud

---

## üìà Sp√©cifications Techniques

### Mod√®le Mistral Install√©

| Propri√©t√© | Valeur |
|-----------|--------|
| **Nom** | mistral:latest |
| **ID** | 6577803aa9a0 |
| **Taille** | 4.4 GB (4,372,824,384 bytes) |
| **Param√®tres** | 7.2B |
| **Quantification** | Q4_K_M |
| **Format** | GGUF |
| **Famille** | llama |
| **Modifi√©** | 2025-11-17 01:32:46 |

### Ollama Service

| Propri√©t√© | Valeur |
|-----------|--------|
| **Version** | 0.12.11 |
| **Port API** | 11434 |
| **Host** | localhost |
| **Endpoint API** | http://localhost:11434/api |
| **D√©marrage** | Automatique (brew services) |

---

## ‚úÖ Statut Final

**Tous les syst√®mes sont op√©rationnels**:

- ‚úÖ Ollama install√© et fonctionnel
- ‚úÖ Mod√®le Mistral t√©l√©charg√© et pr√™t
- ‚úÖ Service Ollama d√©marr√© automatiquement
- ‚úÖ GPT-4o accessible
- ‚úÖ S√©lecteur de mod√®le impl√©ment√©
- ‚úÖ Mode hybride fonctionnel
- ‚úÖ Gestion d'erreurs robuste
- ‚úÖ Interface utilisateur claire
- ‚úÖ Documentation compl√®te

**Pr√™t pour utilisation en production** üöÄ

---

**Derni√®re mise √† jour**: 2025-11-17
**D√©velopp√© par**: Claude Code
**Statut**: ‚úÖ Production Ready
