"""
Tracking prÃ©cis des coÃ»ts d'utilisation des APIs LLM.

Ce module permet de:
- Calculer les coÃ»ts par requÃªte (input + output tokens)
- Cumuler les coÃ»ts par session
- Alerter si seuils dÃ©passÃ©s
- Exporter des rapports de coÃ»ts

Usage:
    from src.utils.cost_tracker import CostTracker

    tracker = CostTracker(config)
    cost = tracker.track_llm_call("gpt-4o", input_tokens=1500, output_tokens=500)
    print(f"Cost: ${cost:.4f}")
    print(f"Total session: ${tracker.get_session_total():.4f}")
"""

from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path
import json

from src.utils.logger import get_logger
from src.utils.exceptions import ConfigurationError

logger = get_logger(__name__)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Dataclasses pour tracking
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class LLMCall:
    """Record d'un appel LLM."""
    timestamp: str
    model: str
    provider: str
    input_tokens: int
    output_tokens: int
    input_cost: float
    output_cost: float
    total_cost: float
    operation: str = "unknown"  # classifier, retriever, generator, etc.
    metadata: Dict = field(default_factory=dict)


@dataclass
class SessionCosts:
    """CoÃ»ts cumulÃ©s d'une session."""
    session_id: str
    start_time: str
    calls: List[LLMCall] = field(default_factory=list)
    total_input_tokens: int = 0
    total_output_tokens: int = 0
    total_cost: float = 0.0
    calls_by_model: Dict[str, int] = field(default_factory=dict)
    costs_by_model: Dict[str, float] = field(default_factory=dict)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Pricing (prix par 1000 tokens)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Pricing par dÃ©faut (janvier 2025)
# Ã€ mettre Ã  jour rÃ©guliÃ¨rement selon les tarifs des providers
DEFAULT_PRICING = {
    # Anthropic Claude
    "claude-sonnet-4": {"input_per_1k": 0.003, "output_per_1k": 0.015},
    "claude-opus-4": {"input_per_1k": 0.015, "output_per_1k": 0.075},
    "claude-haiku-4": {"input_per_1k": 0.00025, "output_per_1k": 0.00125},

    # OpenAI GPT
    "gpt-4o": {"input_per_1k": 0.005, "output_per_1k": 0.015},
    "gpt-4o-mini": {"input_per_1k": 0.00015, "output_per_1k": 0.0006},
    "gpt-4-turbo": {"input_per_1k": 0.01, "output_per_1k": 0.03},
    "gpt-3.5-turbo": {"input_per_1k": 0.0005, "output_per_1k": 0.0015},

    # ModÃ¨les locaux (gratuit)
    "mistral:7b": {"input_per_1k": 0.0, "output_per_1k": 0.0},
    "llama3:70b": {"input_per_1k": 0.0, "output_per_1k": 0.0},
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CostTracker
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class CostTracker:
    """
    Tracker de coÃ»ts pour les appels LLM.

    GÃ¨re le calcul, l'accumulation et les alertes de coÃ»ts.

    Example:
        >>> config = load_config()
        >>> tracker = CostTracker(config)
        >>> cost = tracker.track_llm_call("gpt-4o", 1000, 500, "generator")
        >>> print(f"Cost: ${cost:.4f}")
        Cost: $0.0125
        >>> if tracker.is_over_budget():
        ...     print("Budget exceeded!")
    """

    def __init__(self, config: Optional[object] = None):
        """
        Args:
            config: Objet Config avec costs.pricing et limites
        """
        # Pricing (depuis config ou dÃ©faut)
        if config and hasattr(config, 'costs'):
            self.pricing = config.costs.pricing
            self.track_enabled = config.costs.track_costs
            self.max_cost_per_session = config.costs.max_cost_per_session
            self.alert_threshold = config.costs.alert_threshold
        else:
            self.pricing = DEFAULT_PRICING
            self.track_enabled = True
            self.max_cost_per_session = 1.0
            self.alert_threshold = 0.5

        # Session tracking
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.session = SessionCosts(
            session_id=self.session_id,
            start_time=datetime.now().isoformat()
        )

        # Alertes
        self.alert_sent = False

        logger.info(
            f"CostTracker initialized",
            extra={
                "session_id": self.session_id,
                "max_budget": self.max_cost_per_session,
                "alert_threshold": self.alert_threshold
            }
        )

    def get_model_pricing(self, model: str) -> Tuple[float, float]:
        """
        RÃ©cupÃ¨re les prix input/output pour un modÃ¨le.

        Args:
            model: Nom du modÃ¨le

        Returns:
            Tuple (input_price_per_1k, output_price_per_1k)

        Raises:
            ConfigurationError: Si le modÃ¨le n'est pas dans le pricing
        """
        if model not in self.pricing:
            logger.warning(
                f"Model {model} not in pricing, using default (free)",
                extra={"model": model}
            )
            return (0.0, 0.0)

        pricing = self.pricing[model]
        return (pricing["input_per_1k"], pricing["output_per_1k"])

    def calculate_cost(
        self,
        model: str,
        input_tokens: int,
        output_tokens: int
    ) -> Tuple[float, float, float]:
        """
        Calcule le coÃ»t d'un appel LLM.

        Args:
            model: Nom du modÃ¨le
            input_tokens: Nombre de tokens en entrÃ©e
            output_tokens: Nombre de tokens en sortie

        Returns:
            Tuple (input_cost, output_cost, total_cost)

        Example:
            >>> tracker = CostTracker()
            >>> inp, out, total = tracker.calculate_cost("gpt-4o", 1000, 500)
            >>> print(f"Total: ${total:.4f}")
            Total: $0.0125
        """
        input_price, output_price = self.get_model_pricing(model)

        # Calcul (prix par 1k tokens)
        input_cost = (input_tokens / 1000) * input_price
        output_cost = (output_tokens / 1000) * output_price
        total_cost = input_cost + output_cost

        return (input_cost, output_cost, total_cost)

    def track_llm_call(
        self,
        model: str,
        input_tokens: int,
        output_tokens: int,
        operation: str = "unknown",
        metadata: Optional[Dict] = None
    ) -> float:
        """
        Enregistre un appel LLM et retourne le coÃ»t.

        Args:
            model: Nom du modÃ¨le
            input_tokens: Tokens en entrÃ©e
            output_tokens: Tokens en sortie
            operation: Type d'opÃ©ration (classifier, generator, etc.)
            metadata: MÃ©tadonnÃ©es additionnelles

        Returns:
            CoÃ»t total de l'appel

        Side effects:
            - Met Ã  jour session.total_cost
            - Log le coÃ»t
            - Envoie alerte si seuil dÃ©passÃ©
        """
        if not self.track_enabled:
            return 0.0

        # Calcul coÃ»t
        input_cost, output_cost, total_cost = self.calculate_cost(
            model, input_tokens, output_tokens
        )

        # DÃ©terminer provider depuis model name
        if "claude" in model.lower():
            provider = "anthropic"
        elif "gpt" in model.lower():
            provider = "openai"
        else:
            provider = "local"

        # CrÃ©er record
        call = LLMCall(
            timestamp=datetime.now().isoformat(),
            model=model,
            provider=provider,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            input_cost=input_cost,
            output_cost=output_cost,
            total_cost=total_cost,
            operation=operation,
            metadata=metadata or {}
        )

        # Mettre Ã  jour session
        self.session.calls.append(call)
        self.session.total_input_tokens += input_tokens
        self.session.total_output_tokens += output_tokens
        self.session.total_cost += total_cost

        # Compteurs par modÃ¨le
        self.session.calls_by_model[model] = self.session.calls_by_model.get(model, 0) + 1
        self.session.costs_by_model[model] = self.session.costs_by_model.get(model, 0.0) + total_cost

        # Log
        logger.info(
            f"LLM call tracked: {model} ({operation})",
            extra={
                "model": model,
                "provider": provider,
                "operation": operation,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "cost": total_cost,
                "session_total": self.session.total_cost
            }
        )

        # VÃ©rifier alertes
        self._check_alerts()

        return total_cost

    def _check_alerts(self) -> None:
        """VÃ©rifie et envoie des alertes si seuils dÃ©passÃ©s."""
        current_cost = self.session.total_cost

        # Alerte threshold
        if not self.alert_sent and current_cost >= self.alert_threshold:
            logger.warning(
                f"âš ï¸ Cost alert: ${current_cost:.4f} / ${self.max_cost_per_session:.4f}",
                extra={
                    "current_cost": current_cost,
                    "alert_threshold": self.alert_threshold,
                    "max_budget": self.max_cost_per_session
                }
            )
            self.alert_sent = True

        # Budget dÃ©passÃ©
        if current_cost > self.max_cost_per_session:
            logger.error(
                f"ğŸš¨ Budget exceeded: ${current_cost:.4f} > ${self.max_cost_per_session:.4f}",
                extra={
                    "current_cost": current_cost,
                    "max_budget": self.max_cost_per_session,
                    "overage": current_cost - self.max_cost_per_session
                }
            )

    def is_over_budget(self) -> bool:
        """
        VÃ©rifie si le budget de session est dÃ©passÃ©.

        Returns:
            True si budget dÃ©passÃ©
        """
        return self.session.total_cost > self.max_cost_per_session

    def get_session_total(self) -> float:
        """
        Retourne le coÃ»t total de la session.

        Returns:
            CoÃ»t total en USD
        """
        return self.session.total_cost

    def get_session_stats(self) -> Dict:
        """
        Retourne les statistiques complÃ¨tes de la session.

        Returns:
            Dict avec stats dÃ©taillÃ©es

        Example:
            >>> stats = tracker.get_session_stats()
            >>> print(f"Total calls: {stats['total_calls']}")
            >>> print(f"Total cost: ${stats['total_cost']:.4f}")
        """
        return {
            "session_id": self.session.session_id,
            "start_time": self.session.start_time,
            "total_calls": len(self.session.calls),
            "total_input_tokens": self.session.total_input_tokens,
            "total_output_tokens": self.session.total_output_tokens,
            "total_tokens": self.session.total_input_tokens + self.session.total_output_tokens,
            "total_cost": self.session.total_cost,
            "avg_cost_per_call": (
                self.session.total_cost / len(self.session.calls)
                if self.session.calls else 0.0
            ),
            "calls_by_model": self.session.calls_by_model,
            "costs_by_model": self.session.costs_by_model,
            "is_over_budget": self.is_over_budget()
        }

    def export_session_report(self, output_path: Optional[Path] = None) -> str:
        """
        Exporte un rapport dÃ©taillÃ© de la session.

        Args:
            output_path: Chemin du fichier de sortie (optionnel)
                        Si None, retourne juste le JSON

        Returns:
            JSON string du rapport

        Example:
            >>> tracker.export_session_report(Path("data/logs/cost_report.json"))
        """
        report = {
            "session": asdict(self.session),
            "stats": self.get_session_stats(),
            "pricing_used": self.pricing,
            "generated_at": datetime.now().isoformat()
        }

        report_json = json.dumps(report, indent=2, ensure_ascii=False)

        if output_path:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report_json)
            logger.info(f"Cost report exported to {output_path}")

        return report_json

    def reset_session(self) -> None:
        """Reset le tracking de session (nouveau session_id)."""
        old_session_id = self.session_id
        old_cost = self.session.total_cost

        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.session = SessionCosts(
            session_id=self.session_id,
            start_time=datetime.now().isoformat()
        )
        self.alert_sent = False

        logger.info(
            f"Session reset: {old_session_id} (${old_cost:.4f}) â†’ {self.session_id}",
            extra={"old_session": old_session_id, "old_cost": old_cost}
        )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Fonctions utilitaires
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def estimate_tokens(text: str, model: str = "gpt-4o") -> int:
    """
    Estime le nombre de tokens dans un texte.

    Approximation simple: ~4 caractÃ¨res = 1 token (anglais)
    Pour franÃ§ais: ~5 caractÃ¨res = 1 token

    Pour estimation prÃ©cise, utiliser tiktoken (OpenAI) ou anthropic tokenizer.

    Args:
        text: Texte Ã  estimer
        model: ModÃ¨le (pour choix de tokenizer si disponible)

    Returns:
        Estimation du nombre de tokens

    Example:
        >>> text = "Qu'est-ce qu'une dÃ©rivÃ©e?"
        >>> tokens = estimate_tokens(text)
        >>> print(f"~{tokens} tokens")
        ~6 tokens
    """
    # Approximation simple
    # TODO: Utiliser tiktoken pour prÃ©cision (pip install tiktoken)
    chars = len(text)

    # Heuristique: franÃ§ais ~5 chars/token, anglais ~4 chars/token
    if any(ord(c) > 127 for c in text):  # CaractÃ¨res non-ASCII = probablement franÃ§ais
        return chars // 5
    else:
        return chars // 4


def compare_model_costs(
    models: List[str],
    input_tokens: int,
    output_tokens: int,
    tracker: Optional[CostTracker] = None
) -> Dict[str, float]:
    """
    Compare les coÃ»ts de diffÃ©rents modÃ¨les pour une mÃªme requÃªte.

    Args:
        models: Liste de noms de modÃ¨les
        input_tokens: Tokens en entrÃ©e
        output_tokens: Tokens en sortie
        tracker: CostTracker (optionnel, pour pricing custom)

    Returns:
        Dict {model: cost}

    Example:
        >>> models = ["gpt-4o", "gpt-4o-mini", "claude-sonnet-4"]
        >>> costs = compare_model_costs(models, 1000, 500)
        >>> for model, cost in sorted(costs.items(), key=lambda x: x[1]):
        ...     print(f"{model}: ${cost:.4f}")
        gpt-4o-mini: $0.0004
        claude-sonnet-4: $0.0105
        gpt-4o: $0.0125
    """
    if tracker is None:
        tracker = CostTracker()

    costs = {}
    for model in models:
        _, _, total = tracker.calculate_cost(model, input_tokens, output_tokens)
        costs[model] = total

    return costs


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# NOTES DÃ‰VELOPPEUR
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#
# PRICING:
# - Mis Ã  jour manuellement dans DEFAULT_PRICING
# - Peut Ãªtre overridÃ© via config.yaml
# - VÃ©rifier tarifs rÃ©guliÃ¨rement:
#   * Anthropic: https://www.anthropic.com/pricing
#   * OpenAI: https://openai.com/pricing
#
# PRÃ‰CISION TOKENS:
# - estimate_tokens() utilise approximation simple (chars/4)
# - Pour prÃ©cision: pip install tiktoken
#   ```python
#   import tiktoken
#   enc = tiktoken.encoding_for_model("gpt-4o")
#   tokens = len(enc.encode(text))
#   ```
#
# ALERTES:
# - alert_threshold: Warning log (ex: 50% du budget)
# - max_cost_per_session: Error log si dÃ©passÃ©
# - Pas de blocage automatique (Ã  implÃ©menter si nÃ©cessaire)
#
# EXPORT:
# - export_session_report() gÃ©nÃ¨re JSON complet
# - Peut Ãªtre parsÃ© pour analytics, facturation, etc.
# - Inclut tous les calls individuels + agrÃ©gations
#
# SESSION:
# - Une session = durÃ©e de vie du tracker
# - reset_session() pour recommencer Ã  zÃ©ro
# - session_id = timestamp pour unicitÃ©
#
# USAGE RECOMMANDÃ‰:
# 1. CrÃ©er CostTracker au dÃ©marrage
# 2. Passer Ã  tous les agents
# 3. Appeler track_llm_call() aprÃ¨s chaque appel LLM
# 4. VÃ©rifier is_over_budget() avant appels coÃ»teux
# 5. Exporter rapport en fin de session
#
# OPTIMISATION COÃ›TS:
# - Utiliser GPT-4o-mini pour dev/tests
# - Utiliser Claude Haiku pour tÃ¢ches simples (classification)
# - Mettre en cache les rÃ©ponses frÃ©quentes
# - Limiter max_tokens dans config
# - Utiliser modÃ¨les locaux (Ollama) pour dev
#
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
